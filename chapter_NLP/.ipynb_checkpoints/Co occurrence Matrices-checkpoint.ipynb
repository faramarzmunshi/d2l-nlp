{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurrence matrices with a fixed context window\n",
    "\n",
    "The big idea – Similar words tend to occur together and will have similar context for example – Guava is a fruit. Grapefruit is a fruit.\n",
    "Guava and Grapefruit tend to have a similar context i.e fruit.\n",
    "\n",
    "Before we dive into the details of how a co-occurrence matrix is constructed, there are two concepts that need to be clarified – what do Co-Occurrence and Context Window mean here?\n",
    "\n",
    "Co-occurrence – For a given corpus, the co-occurrence of a pair of words say w1 and w2 is the number of times they have appeared together in a Context Window.\n",
    "\n",
    "Context Window – Context window is specified by a number and the direction. So what does a context window of 2 (around) means? Let us see an example below,\n",
    "\n",
    " \n",
    "\n",
    "| **Fabulous**\t| **Fairytale**\t| _Fox_ | **Flew** | **Far** | From |\tFive | Feet | Forwards |\n",
    "|---|:--:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "\n",
    "The words \"Fabulous\", \"Fairytale\", \"Flew\", and \"Far\" (bolded) are a 2 (around) context window for the word ‘Fox’ and for calculating the co-occurrence only these words will be counted. \n",
    "\n",
    "Now, let us take an example corpus to calculate a co-occurrence matrix.\n",
    "\n",
    "Corpus = He is not lazy. He is intelligent. He is smart.\n",
    "\n",
    " \n",
    "\n",
    "| - | He | is  | not  | lazy | intelligent | smart |\n",
    "|--- |:--:|:-:  |:-:   |:-:   |:-:          |:-:    |\n",
    "| He | 0 | \t4\t| 2 | 1 | 2 | 1 |\n",
    "| is | 4 | 0  | 1 | 2  | 2 | 1 | \n",
    "| not | 2 | 1 | 0 | 1 | 0 | 0 | \n",
    "| lazy\t | 1 | 2 | 1 | 0 | 0 | 0 |\n",
    "| intelligent |\t2 | 2 | 0 | 0 | 0 | 0 |\n",
    "| smart | 1 | 1 | 0 | 0 | 0 | 0 |\n",
    "\n",
    "\n",
    "Let us understand this co-occurrence matrix by seeing two examples in the table above.\n",
    "\n",
    "The box under \"is and to the right of He is the number of times ‘He’ and ‘is’ have appeared in the context window 2 and it can be seen that the count turns out to be 4. The below table will help you visualise the count.\n",
    "\n",
    "while the word ‘lazy’ has never appeared with ‘intelligent’ in the context window and therefore has been assigned 0 in the box below intelligent and to the right of lazy.\n",
    "\n",
    "### So... what now — after you have vectorized the text?\n",
    "\n",
    "Numerical representations allow us to use numerical classification methods to analyze our text. If we want to build a document classifier, we can obtain document level vectors for each review and use them as feature vectors to predict the class label. We can then plug this data into any type of classifier.\n",
    "\n",
    "By one hot encoding the classes, we can plug this data into any type of classifier\n",
    "Textual data is rich and high in dimensions, which makes more complex classifiers such as neural networks ideal for NLP.\n",
    "\n",
    "To give another simple example of one-hot encoding They were represented as one-hot vectors, the count of unique vocabulary in the corpus as the length of the vector, and the place in which the vocabulary occurred first as the position in the one-hot vector being a 1. For example, if my vocabulary was the following sentence:\n",
    "\n",
    "> The boy liked the turtles.\n",
    "\n",
    "The length of the above sentence is five, but there are four unique words in the vocabulary, \"the\", \"boy\", \"liked\", and \"turtles.\" So the length of our embedding vectors for each would be four. For \"the,\" the embedding would look like `[1, 0, 0, 0]`, the embedding for boy would be `[0, 1, 0, 0]`, the embedding for liked would be `[0, 0, 1, 0]`. The embedding for the next \"the\" would reuse the one-hot vector embedding used previously, and default to `[1, 0, 0, 0]` for every re-occurrence of \"the,\" and lastly, the embedding for turtles would be `[0, 0, 0, 1]`. But according to the questions we posed earlier, these don't represent any of the features of syntactic or semantic similarity; these only represent the most basic feature of whether or not a word occurred or not. In addition, any calculations or computation that one would like to do with these one-hot vectors would be a problem as the inherent sparsity of these vectors makes it increasingly inefficient as the vocabulary size increases.\n",
    "\n",
    "Fortunately, it turns out that a number of efficient techniques\n",
    "can quickly discover broadly useful word embeddings in an *unsupervised* manner.\n",
    "\n",
    "These embeddings map each word onto a low-dimensional vector $w \\in R^d$ with $d$ commonly chosen to be roughly $100$.\n",
    "Intuitively, these embeddings are chosen based on the contexts in which words appear.\n",
    "Words that appear in similar contexts, like \"tennis\" and \"racquet,\" should have similar embeddings\n",
    "while words that are not alike, like \"rat\" and \"gourmet,\" should have dissimilar embeddings.\n",
    "\n",
    "We will explore the much more complex set of embeddings created using shallow neural networks by focusing on word2vec models. Trained over large corpora, word2vec uses unsupervised learning to determine semantic and syntactic meaning from word co-occurrence, which is used to construct vector representations for every word in the vocabulary.\n",
    "\n",
    "Word2vec was developed at Google by a research team led by Tomas Mikolov. The research paper takes a while to read, but is worth the time and effort.\n",
    "\n",
    "The model uses a two layer shallow neural network to find the vector mappings for each word in the corpus. The neural network is used to predict known co-occurrences in the corpus and the weights of the hidden layer are used to create the word vectors. Somewhat surprisingly, word vectors created using this method preserve many of the linear regularities found in language.\n",
    "\n",
    "The Efficient Estimation of Word Representations in Vector Space paper shares the following result:\n",
    "“Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector(”King”) — vector(”Man”) + vector(”Woman”) results in a vector that is closest to the vector representation of the word Queen.”\n",
    "\n",
    "There are two model architectures used to train word2vec: Continuous Bag of Words and Skip Gram. These models determine how textual data is passed into the neural network. Both of these architectures use a context window to determine contextually similar words. A context window with a fixed size n means that all words within n units from the target word belong to its context.\n",
    "\n",
    "Consider the following example with a fixed window size of 2:\n",
    "\n",
    "> \"The quick brown fox jumped over the lazy dog.\"\n",
    "\n",
    "Fox is our target word and quick, brown, jumped, over belong to the context of fox. The assumption is that with enough examples of contextual similarity, the network is able to learn the correct associations between words.\n",
    "This assumption is in line with the distributional hypothesis we presented earlier, which states that “words which are used and occur in the same contexts tend to purport similar meaning.”\n",
    "\n",
    "The implementation of context window in word2vec is dynamic.\n",
    "A dynamic context window has a maximum window size. Context is sampled from the maximum window size with probability 1/d, where d is the distance between the word to the target.\n",
    "Consider the target word fox using a dynamic context window with maximum window size of 2. (brown, jumped) have a 1/1 probability of being included in the context since they are one word away from fox. (quick, over) have a 1/2 probability of being included in the context since they are two words away from fox.\n",
    "Using this concept, the Continuous Bag of Words and the Skip Gram model separates data into observations of target words and their context.\n",
    "\n",
    "### Continuous Bag of Words\n",
    "\n",
    "We structure the data such that the context is used to predict the target word. For example, if our context is (quick, brown, jumped, over), we use that as features of the class fox.\n",
    "\n",
    "### Skip Gram\n",
    "\n",
    "We structure the data such that the target word is used to predict the context. For example, we use the feature (fox) to predict the context (quick, brown, jumped, over).\n",
    "\n",
    "### Building the Neural Network\n",
    "\n",
    "Word2vec trains a shallow neural network over data as structured using either Continuous Bag of Words or Skip Gram architecture. Instead of leveraging the model for predictive purposes, we use the hidden weights from the neural network to generate the word vectors.\n",
    "Assuming a Continuous Bag of Words architecture with a fixed context window of 1 word, this is what the process would look like. First, the corpus.\n",
    "\n",
    "> I like math\n",
    "> I like programming\n",
    "> Today is Friday\n",
    "> Today is a good day\n",
    "\n",
    "To make things even easier, we can require our context window to only include words which proceeds the target. We can assume that the context of words at the end of a sentence is the first word of the next sentence. Under such rules:\n",
    "\n",
    "- like is the context of target I\n",
    "- math is the context of target like\n",
    "- programming is also the context of target like\n",
    "\n",
    "Even with such a simple corpus, we can begin to recognize some patterns. “Math” and “programming” are both context to “like”. While this might not be picked up by the model, both of these words can be understood as things that I like.\n",
    "\n",
    "#### Step 1\n",
    "The first step is to one hot encode our classes like we did above with the 'I like turtles' example (the words in our vocabulary): I, like, math, programming, today, is, Friday, a, good, day\n",
    "\n",
    "#### Step 2\n",
    "Create a feed forward neural network with one hidden layer and an output layer using the softmax activation function. The data set used to train the network uses the one hot encoded context vector to predict the one hot encoded target vector.\n",
    "The number of neurons in the hidden layer corresponds to the number of dimensions in the final word vectors.\n",
    "\n",
    "#### Step 3\n",
    "Obtain the weights of the hidden network. Each row in the weight matrix corresponds to the vector of each word in the vocabulary.\n",
    "\n",
    "Realistically, this is not something that we do very often. Good word2vec models require a very large corpus in the billions of words. Fortunately, pre-trained models are easy to use and find. You can download the word2vec model trained over the 100 billion word Google News corpus on their website, or you can use GluonNLP to load a set of pre-trained word embedding.\n",
    "\n",
    "Here, we'll show you how to create the model and train it, but, in the end, will use pre-built word embeddings that have been independently verified for accuracy for testing and understanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
