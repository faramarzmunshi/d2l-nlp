{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-occurrence matrices with Skip-Gram and CBOW\n",
    "\n",
    "The big idea – Similar words tend to occur together and will have similar context for example – Guava is a fruit. Grapefruit is a fruit.\n",
    "Guava and Grapefruit tend to have a similar context i.e fruit.\n",
    "\n",
    "Before we dive into the details of how a co-occurrence matrix is constructed, there are two concepts that need to be clarified – what do Co-Occurrence and Context Window mean here?\n",
    "\n",
    "Co-occurrence – For a given corpus, the co-occurrence of a pair of words say w1 and w2 is the number of times they have appeared together in a Context Window.\n",
    "\n",
    "Context Window – Context window is specified by a number and the direction. So what does a context window of 2 (around) means? Let us see an example below,\n",
    "\n",
    " \n",
    "\n",
    "| **Fabulous**\t| **Fairytale**\t| _Fox_ | **Flew** | **Far** | From |\tFive | Feet | Forwards |\n",
    "|---|:--:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "\n",
    "The words \"Fabulous\", \"Fairytale\", \"Flew\", and \"Far\" (bolded) are a 2 (around) context window for the word ‘Fox’ and for calculating the co-occurrence only these words will be counted. \n",
    "\n",
    "Now, let us take an example corpus to calculate a co-occurrence matrix.\n",
    "\n",
    "Corpus = He is not lazy. He is intelligent. He is smart.\n",
    "\n",
    " \n",
    "\n",
    "| - | He | is  | not  | lazy | intelligent | smart |\n",
    "|--- |:--:|:-:  |:-:   |:-:   |:-:          |:-:    |\n",
    "| He | 0 | \t4\t| 2 | 1 | 2 | 1 |\n",
    "| is | 4 | 0  | 1 | 2  | 2 | 1 | \n",
    "| not | 2 | 1 | 0 | 1 | 0 | 0 | \n",
    "| lazy\t | 1 | 2 | 1 | 0 | 0 | 0 |\n",
    "| intelligent |\t2 | 2 | 0 | 0 | 0 | 0 |\n",
    "| smart | 1 | 1 | 0 | 0 | 0 | 0 |\n",
    "\n",
    "\n",
    "Let us understand this co-occurrence matrix by examining two examples in the table above.\n",
    "\n",
    "The box under \"is\" and to the right of \"He\" is the number of times ‘He’ and ‘is’ have appeared in the context window 2 and it can be seen that the count turns out to be 4, while the word ‘lazy’ has never appeared with ‘intelligent’ in the context window and therefore has been assigned 0 in the box below intelligent and to the right of lazy.\n",
    "\n",
    "## Dealing with the cooccurrence matrices\n",
    "\n",
    "Numerical representations allow us to use numerical classification methods to analyze our text. If we want to build a document classifier, we can obtain document level vectors for each review and use them as feature vectors to predict the class label. We can then plug this data into any type of classifier. In the last section, we discussed one-hot encoding and document level word embeddings.\n",
    "\n",
    "By one hot encoding the classes, we can plug this data into any type of classifier\n",
    "Textual data is rich and high in dimensions, which makes more complex classifiers such as neural networks ideal for NLP.\n",
    "\n",
    "Let's take a look at this simple example of one-hot encoding once again. The count of unique vocabulary in the corpus is the length of the vector, and the place in which the vocabulary occurred first as the position in the one-hot vector being a 1. For example, if my vocabulary was the following sentence:\n",
    "\n",
    "> The boy liked the turtles.\n",
    "\n",
    "The length of the above sentence is five, but there are four unique words in the vocabulary, \"the\", \"boy\", \"liked\", and \"turtles.\" So the length of our embedding vectors for each would be four. For \"the,\" the embedding would look like `[1, 0, 0, 0]`, the embedding for boy would be `[0, 1, 0, 0]`, the embedding for liked would be `[0, 0, 1, 0]`. The embedding for the next \"the\" would reuse the one-hot vector embedding used previously, and default to `[1, 0, 0, 0]` for every re-occurrence of \"the,\" and lastly, the embedding for turtles would be `[0, 0, 0, 1]`. These don't represent any of the features of syntactic or semantic similarity; these only represent the most basic feature of whether or not a word occurred or not. The same happens with the TF-IDF implementation of word embeddings in the last notebook. Any calculations or computation that one would like to do with these one-hot vectors or TF-IDF vectors would be a problem as the inherent sparsity of these vectors makes it increasingly inefficient as the vocabulary size increases. That's not to say that they haven't seen any success, but the success is highly limited and easily improvable.\n",
    "\n",
    "Fortunately, it turns out that a number of efficient techniques\n",
    "can quickly discover broadly useful word embeddings in an *unsupervised* manner.\n",
    "\n",
    "These embeddings map each word onto a low-dimensional vector $w \\in R^d$ with $d$ commonly chosen to be roughly $100$.\n",
    "Intuitively, these embeddings are chosen based on the contexts in which words appear.\n",
    "Words that appear in similar contexts, like \"tennis\" and \"racquet,\" should have similar embeddings\n",
    "while words that are not alike, like \"rat\" and \"gourmet,\" should have dissimilar embeddings.\n",
    "\n",
    "We will explore the much more complex set of embeddings created using shallow neural networks by focusing on word2vec models. Trained over large corpora, word2vec uses unsupervised learning to determine semantic and syntactic meaning from word co-occurrence from the matrices we described in this notebook, which is used to construct vector representations for every word in the vocabulary.\n",
    "\n",
    "Word2vec was developed at Google by a research team led by Tomas Mikolov. The research paper covers the topics in depth and can be found (here)[https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf].\n",
    "\n",
    "The model uses a two layer shallow neural network to find the vector mappings for each word in the corpus. The neural network is used to predict known co-occurrences in the corpus and the weights of the hidden layer are used to create the word vectors. Somewhat surprisingly, word vectors created using this method preserve many of the linear regularities found in language.\n",
    "\n",
    "The Efficient Estimation of Word Representations in Vector Space paper shares the following result:\n",
    "\n",
    "“Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector(”King”) — vector(”Man”) + vector(”Woman”) results in a vector that is closest to the vector representation of the word Queen.”\n",
    "\n",
    "There are two model architectures used to train word2vec: Continuous Bag of Words and Skip Gram. These models determine how textual data is passed into the neural network. Both of these architectures use a context window to determine contextually similar words. A context window with a fixed size n means that all words within n units from the target word belong to its context like mentioned above. To further clarify and introduce the concept of dynamic contextual windows, let's consider the following sentence:\n",
    "\n",
    "> \"The quick brown fox jumped over the lazy dog.\"\n",
    "\n",
    "Fox is our target word and quick, brown, jumped, over belong to the context of fox. The assumption is that with enough examples of contextual similarity, the network is able to learn the correct associations between the words.\n",
    "This assumption is in line with the distributional hypothesis we presented in the last notebook, which states that “words which are used and occur in the same contexts tend to purport similar meaning.”\n",
    "\n",
    "The implementation of context window in word2vec though, is dynamic. A dynamic context window has a maximum window size. Context is sampled from the maximum window size with probability $1/d$, where `d` is the distance between the word to the target.\n",
    "\n",
    "Consider the target word fox using a dynamic context window with maximum window size of 2. (brown, jumped) have a 1/1 probability of being included in the context since they are one word away from fox. (quick, over) have a 1/2 probability of being included in the context since they are two words away from fox.\n",
    "\n",
    "Using this concept, the Continuous Bag of Words (BOW) and the Skip Gram model separates data into observations of target words and their context. Let's further dive into depth of firstly, the skip-gram model, followed by the BOW model in its continuous form (CBOW)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Skip-Gram Model\n",
    "\n",
    "The skip-gram model assumes that a word can be used to generate the words that surround it in a text sequence. For example, we assume that the text sequence is \"the\", \"man\", \"loves\", \"his\", and \"son\". We use \"loves\" as the central target word and set the context window size to 2. As shown in Figure 12.1, given the central target word \"loves\", the skip-gram model is concerned with the conditional probability for generating the context words, \"the\", \"man\", \"his\" and \"son\", that are within a distance of no more than 2 words, which is\n",
    "\n",
    "$$\\mathbb{P}(\\textrm{\"the\"},\\textrm{\"man\"},\\textrm{\"his\"},\\textrm{\"son\"}\\mid\\textrm{\"loves\"}).$$\n",
    "\n",
    "We assume that, given the central target word, the context words are generated independently of each other. In this case, the formula above can be rewritten as\n",
    "\n",
    "$$\\mathbb{P}(\\textrm{\"the\"}\\mid\\textrm{\"loves\"})\\cdot\\mathbb{P}(\\textrm{\"man\"}\\mid\\textrm{\"loves\"})\\cdot\\mathbb{P}(\\textrm{\"his\"}\\mid\\textrm{\"loves\"})\\cdot\\mathbb{P}(\\textrm{\"son\"}\\mid\\textrm{\"loves\"}).$$\n",
    "\n",
    "![The skip-gram model cares about the conditional probability of generating context words for a given central target word. ](../img/skip-gram.svg)\n",
    "\n",
    "\n",
    "In the skip-gram model, each word is represented as two $d$-dimension vectors, which are used to compute the conditional probability. We assume that the word is indexed as $i$ in the dictionary, its vector is represented as $\\mathbf{v}_i\\in\\mathbb{R}^d$ when it is the central target word, and $\\mathbf{u}_i\\in\\mathbb{R}^d$ when it is a context word.  Let the central target word $w_c$ and context word $w_o$ be indexed as $c$ and $o$ respectively in the dictionary. The conditional probability of generating the context word for the given central target word can be obtained by performing a softmax operation on the vector inner product:\n",
    "\n",
    "$$\\mathbb{P}(w_o \\mid w_c) = \\frac{\\text{exp}(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_i^\\top \\mathbf{v}_c)},$$\n",
    "\n",
    "where vocabulary index set $\\mathcal{V} = \\{0, 1, \\ldots, |\\mathcal{V}|-1\\}$. Assume that a text sequence of length $T$ is given, where the word at time step $t$ is denoted as $w^{(t)}$. Assume that context words are independently generated given center words. When context window size is $m$, the likelihood function of the skip-gram model is the joint probability of generating all the context words given any center word\n",
    "\n",
    "$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} \\mathbb{P}(w^{(t+j)} \\mid w^{(t)}),$$\n",
    "\n",
    "Here, any time step that is less than 1 or greater than $T$ can be ignored.\n",
    "\n",
    "### Skip-Gram Model Training\n",
    "\n",
    "The skip-gram model parameters are the central target word vector and context word vector for each individual word.  In the training process, we are going to learn the model parameters by maximizing the likelihood function, which is also known as maximum likelihood estimation. This is equivalent to minimizing the following loss function:\n",
    "\n",
    "$$ - \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m,\\ j \\neq 0} \\text{log}\\, \\mathbb{P}(w^{(t+j)} \\mid w^{(t)}).$$\n",
    "\n",
    "\n",
    "If we use the SGD, in each iteration we are going to pick a shorter subsequence through random sampling to compute the loss for that subsequence, and then compute the gradient to update the model parameters. The key of gradient computation is to compute the gradient of the logarithmic conditional probability for the central word vector and the context word vector. By definition, we first have\n",
    "\n",
    "\n",
    "$$\\log \\mathbb{P}(w_o \\mid w_c) =\n",
    "\\mathbf{u}_o^\\top \\mathbf{v}_c - \\log\\left(\\sum_{i \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_i^\\top \\mathbf{v}_c)\\right)$$\n",
    "\n",
    "Through differentiation, we can get the gradient $\\mathbf{v}_c$ from the formula above.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\text{log}\\, \\mathbb{P}(w_o \\mid w_c)}{\\partial \\mathbf{v}_c}\n",
    "&= \\mathbf{u}_o - \\frac{\\sum_{j \\in \\mathcal{V}} \\exp(\\mathbf{u}_j^\\top \\mathbf{v}_c)\\mathbf{u}_j}{\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)}\\\\\n",
    "&= \\mathbf{u}_o - \\sum_{j \\in \\mathcal{V}} \\left(\\frac{\\text{exp}(\\mathbf{u}_j^\\top \\mathbf{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_i^\\top \\mathbf{v}_c)}\\right) \\mathbf{u}_j\\\\\n",
    "&= \\mathbf{u}_o - \\sum_{j \\in \\mathcal{V}} \\mathbb{P}(w_j \\mid w_c) \\mathbf{u}_j.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Its computation obtains the conditional probability for all the words in the dictionary given the central target word $w_c$. We then use the same method to obtain the gradients for other word vectors.\n",
    "\n",
    "After the training, for any word in the dictionary with index $i$, we are going to get its two word vector sets $\\mathbf{v}_i$ and $\\mathbf{u}_i$.  In applications of natural language processing (NLP), the central target word vector in the skip-gram model is generally used as the representation vector of a word.\n",
    "\n",
    "\n",
    "## The Continuous Bag Of Words (CBOW) Model\n",
    "\n",
    "The continuous bag of words (CBOW) model is similar to the skip-gram model. The biggest difference is that the CBOW model assumes that the central target word is generated based on the context words before and after it in the text sequence. With the same text sequence \"the\", \"man\", \"loves\", \"his\" and \"son\", in which \"loves\" is the central target word, given a context window size of 2, the CBOW model is concerned with the conditional probability of generating the target word \"loves\" based on the context words \"the\", \"man\", \"his\" and \"son\"(as shown in Figure 12.2), such as\n",
    "\n",
    "$$\\mathbb{P}(\\textrm{\"loves\"}\\mid\\textrm{\"the\"},\\textrm{\"man\"},\\textrm{\"his\"},\\textrm{\"son\"}).$$\n",
    "\n",
    "![The CBOW model cares about the conditional probability of generating the central target word from given context words.  ](../img/cbow.svg)\n",
    "\n",
    "Since there are multiple context words in the CBOW model, we will average their word vectors and then use the same method as the skip-gram model to compute the conditional probability. We assume that $\\mathbf{v_i}\\in\\mathbb{R}^d$ and $\\mathbf{u_i}\\in\\mathbb{R}^d$ are the context word vector and central target word vector of the word with index $i$ in the dictionary (notice that the symbols are opposite to the ones in the skip-gram model). Let central target word $w_c$ be indexed as $c$, and context words $w_{o_1}, \\ldots, w_{o_{2m}}$ be indexed as $o_1, \\ldots, o_{2m}$ in the dictionary. Thus, the conditional probability of generating a central target word from the given context word is\n",
    "\n",
    "$$\\mathbb{P}(w_c \\mid w_{o_1}, \\ldots, w_{o_{2m}}) = \\frac{\\text{exp}\\left(\\frac{1}{2m}\\mathbf{u}_c^\\top (\\mathbf{v}_{o_1} + \\ldots + \\mathbf{v}_{o_{2m}}) \\right)}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}\\left(\\frac{1}{2m}\\mathbf{u}_i^\\top (\\mathbf{v}_{o_1} + \\ldots + \\mathbf{v}_{o_{2m}}) \\right)}.$$\n",
    "\n",
    "\n",
    "For brevity, denote $\\mathcal{W}_o= \\{w_{o_1}, \\ldots, w_{o_{2m}}\\}$, and $\\bar{\\mathbf{v}}_o = \\left(\\mathbf{v}_{o_1} + \\ldots + \\mathbf{v}_{o_{2m}} \\right)/(2m)$. The equation above can be simplified as\n",
    "\n",
    "$$\\mathbb{P}(w_c \\mid \\mathcal{W}_o) = \\frac{\\exp\\left(\\mathbf{u}_c^\\top \\bar{\\mathbf{v}}_o\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o\\right)}.$$\n",
    "\n",
    "Given a text sequence of length $T$, we assume that the word at time step $t$ is $w^{(t)}$, and the context window size is $m$.  The likelihood function of the CBOW model is the probability of generating any central target word from the context words.\n",
    "\n",
    "$$ \\prod_{t=1}^{T}  \\mathbb{P}(w^{(t)} \\mid  w^{(t-m)}, \\ldots,  w^{(t-1)},  w^{(t+1)}, \\ldots,  w^{(t+m)}).$$\n",
    "\n",
    "### CBOW Model Training\n",
    "\n",
    "CBOW model training is quite similar to skip-gram model training.  The maximum likelihood estimation of the CBOW model is equivalent to minimizing the loss function.\n",
    "\n",
    "$$  -\\sum_{t=1}^T  \\text{log}\\, \\mathbb{P}(w^{(t)} \\mid  w^{(t-m)}, \\ldots,  w^{(t-1)},  w^{(t+1)}, \\ldots,  w^{(t+m)}).$$\n",
    "\n",
    "Notice that\n",
    "\n",
    "$$\\log\\,\\mathbb{P}(w_c \\mid \\mathcal{W}_o) = \\mathbf{u}_c^\\top \\bar{\\mathbf{v}}_o - \\log\\,\\left(\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o\\right)\\right).$$\n",
    "\n",
    "Through differentiation, we can compute the logarithm of the conditional probability of the gradient of any context word vector $\\mathbf{v}_{o_i}$($i = 1, \\ldots, 2m$) in the formula above.\n",
    "\n",
    "$$\\frac{\\partial \\log\\, \\mathbb{P}(w_c \\mid \\mathcal{W}_o)}{\\partial \\mathbf{v}_{o_i}} = \\frac{1}{2m} \\left(\\mathbf{u}_c - \\sum_{j \\in \\mathcal{V}} \\frac{\\exp(\\mathbf{u}_j^\\top \\bar{\\mathbf{v}}_o)\\mathbf{u}_j}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o)} \\right) = \\frac{1}{2m}\\left(\\mathbf{u}_c - \\sum_{j \\in \\mathcal{V}} \\mathbb{P}(w_j \\mid \\mathcal{W}_o) \\mathbf{u}_j \\right).$$\n",
    "\n",
    "We then use the same method to obtain the gradients for other word vectors. Unlike the skip-gram model, we usually use the context word vector as the representation vector for a word in the CBOW model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
